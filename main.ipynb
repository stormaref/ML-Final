{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preaparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gun violance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_context_spam = pd.read_csv('dataset/gun-violence/context-spam/published_data_spam-MLJ-2022_gun-violence_context-spam_context_spam_5000_no_link.csv')\n",
    "gun_not_context_spam = pd.read_csv('dataset/gun-violence/context-spam/published_data_spam-MLJ-2022_gun-violence_context-spam_not_context_spam_5000_no_link.csv')\n",
    "gun_context_spam.drop(['tweet_id'], axis=1, inplace=True)\n",
    "gun_not_context_spam.drop(['tweet_id'], axis=1, inplace=True)\n",
    "gun = pd.concat([gun_context_spam, gun_not_context_spam])\n",
    "gun.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeToo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metoo_context_spam = pd.read_csv('dataset/metoo/context-spam/published_data_spam-MLJ-2022_metoo_context-spam_context_spam_5000_no_link.csv')\n",
    "metoo_not_context_spam = pd.read_csv('dataset/metoo/context-spam/published_data_spam-MLJ-2022_metoo_context-spam_not_context_spam_5000_no_link.csv')\n",
    "metoo_context_spam.drop(['tweet_id'], axis=1, inplace=True)\n",
    "metoo_not_context_spam.drop(['tweet_id'], axis=1, inplace=True)\n",
    "metoo = pd.concat([metoo_context_spam, metoo_not_context_spam])\n",
    "metoo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parenting_context_spam = pd.read_csv('dataset/parenting/context-spam/published_data_spam-MLJ-2022_parenting_context-spam_context_spam_5000_no_link.csv')\n",
    "parenting_not_context_spam = pd.read_csv('dataset/parenting/context-spam/published_data_spam-MLJ-2022_parenting_context-spam_not_context_spam_5000_no_link.csv')\n",
    "parenting_context_spam.drop(['tweet_id'], axis=1, inplace=True)\n",
    "parenting_not_context_spam.drop(['tweet_id'], axis=1, inplace=True)\n",
    "parenting = pd.concat([parenting_context_spam, parenting_not_context_spam])\n",
    "parenting.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"<em>.*?</em>\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "    cleaned_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun['text'] = gun['text'].apply(clean_text)\n",
    "metoo['text'] = metoo['text'].apply(clean_text)\n",
    "parenting['text'] = parenting['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_train, gun_test = train_test_split(\n",
    "    gun, test_size=0.1, random_state=42, stratify=gun['label'])\n",
    "gun_train, gun_val = train_test_split(\n",
    "    gun_train, test_size=0.1, random_state=42, stratify=gun_train['label'])\n",
    "\n",
    "gun_train = pd.DataFrame(gun_train, columns=gun.columns)\n",
    "gun_test = pd.DataFrame(gun_test, columns=gun.columns)\n",
    "gun_val = pd.DataFrame(gun_val, columns=gun.columns)\n",
    "\n",
    "metoo_train, metoo_test = train_test_split(\n",
    "    metoo, test_size=0.1, random_state=42, stratify=metoo['label'])\n",
    "metoo_train, metoo_val = train_test_split(\n",
    "    metoo_train, test_size=0.1, random_state=42, stratify=metoo_train['label'])\n",
    "\n",
    "metoo_train = pd.DataFrame(metoo_train, columns=metoo.columns)\n",
    "metoo_test = pd.DataFrame(metoo_test, columns=metoo.columns)\n",
    "metoo_val = pd.DataFrame(metoo_val, columns=metoo.columns)\n",
    "\n",
    "parenting_train, parenting_test = train_test_split(\n",
    "    parenting, test_size=0.1, random_state=42, stratify=parenting['label'])\n",
    "parenting_train, parenting_val = train_test_split(\n",
    "    parenting_train, test_size=0.1, random_state=42, stratify=parenting_train['label'])\n",
    "\n",
    "parenting_train = pd.DataFrame(parenting_train, columns=parenting.columns)\n",
    "parenting_test = pd.DataFrame(parenting_test, columns=parenting.columns)\n",
    "parenting_val = pd.DataFrame(parenting_val, columns=parenting.columns)\n",
    "\n",
    "gun_train.reset_index(drop=True, inplace=True)\n",
    "gun_test.reset_index(drop=True, inplace=True)\n",
    "gun_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "metoo_train.reset_index(drop=True, inplace=True)\n",
    "metoo_test.reset_index(drop=True, inplace=True)\n",
    "metoo_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "parenting_train.reset_index(drop=True, inplace=True)\n",
    "parenting_test.reset_index(drop=True, inplace=True)\n",
    "parenting_val.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parenting_train_bow = vectorizer.fit_transform(parenting_train['text'])\n",
    "parenting_test_bow = vectorizer.transform(parenting_test['text'])\n",
    "parenting_train_bow = parenting_train_bow.toarray()\n",
    "parenting_test_bow = parenting_test_bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_train_bow = vectorizer.fit_transform(gun_train['text'])\n",
    "gun_test_bow = vectorizer.transform(gun_test['text'])\n",
    "gun_train_bow = gun_train_bow.toarray()\n",
    "gun_test_bow = gun_test_bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeToo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metoo_train_bow = vectorizer.fit_transform(metoo_train['text'])\n",
    "metoo_test_bow = vectorizer.transform(metoo_test['text'])\n",
    "metoo_train_bow = metoo_train_bow.toarray()\n",
    "metoo_test_bow = metoo_test_bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [2, 4, 8, 16],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_model = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(parenting_train_bow, parenting_train['label'])\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Parameters:', best_params)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "parenting_rf_pred = best_model.predict(parenting_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(gun_train_bow, gun_train['label'])\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Parameters:', best_params)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "gun_rf_pred = best_model.predict(gun_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeToo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(metoo_train_bow, metoo_train['label'])\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print('Best Parameters:', best_params)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "metoo_rf_pred = best_model.predict(metoo_test_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.bert_model.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bert_model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        bert_output = last_hidden_state[:, 0, :]\n",
    "        dropout_output = self.dropout(bert_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        return\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'text']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "        tokenized = self.tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "        output = {\n",
    "            'input_ids': tokenized['input_ids'].flatten(),\n",
    "            'attention_mask': tokenized['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gun_train_dataset = CustomDataset(gun_train, bert_tokenizer)\n",
    "gun_val_dataset = CustomDataset(gun_val, bert_tokenizer)\n",
    "gun_test_dataset = CustomDataset(gun_test, bert_tokenizer)\n",
    "\n",
    "metoo_train_dataset = CustomDataset(metoo_train, bert_tokenizer)\n",
    "metoo_val_dataset = CustomDataset(metoo_val, bert_tokenizer)\n",
    "metoo_test_dataset = CustomDataset(metoo_test, bert_tokenizer)\n",
    "\n",
    "parenting_train_dataset = CustomDataset(parenting_train, bert_tokenizer)\n",
    "parenting_val_dataset = CustomDataset(parenting_val, bert_tokenizer)\n",
    "parenting_test_dataset = CustomDataset(parenting_test, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 22\n",
    "num_epochs = 20\n",
    "\n",
    "gun_train_dataloader = torch.utils.data.DataLoader(gun_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "gun_val_dataloader = torch.utils.data.DataLoader(gun_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "gun_test_dataloader = torch.utils.data.DataLoader(gun_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "metoo_train_dataloader = torch.utils.data.DataLoader(metoo_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "metoo_val_dataloader = torch.utils.data.DataLoader(metoo_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "metoo_test_dataloader = torch.utils.data.DataLoader(metoo_test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "parenting_train_dataloader = torch.utils.data.DataLoader(parenting_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "parenting_val_dataloader = torch.utils.data.DataLoader(parenting_val_dataset, batch_size=batch_size, shuffle=True)\n",
    "parenting_test_dataloader = torch.utils.data.DataLoader(parenting_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(bert_model, train_dataloader, validation_dataloader) -> MyModel:\n",
    "    bert_model = bert_model.to(device)\n",
    "    my_model = MyModel(bert_model)\n",
    "    my_model = my_model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(my_model.parameters(), lr=0.000005)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(20):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            inputs = {key: value.to(device) for key, value in batch.items() if key != 'label'}\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = my_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            predictions = torch.argmax(probabilities, dim=1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.shape[0]\n",
    "            inputs = None\n",
    "            labels = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dataloader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        my_model.eval()\n",
    "        val_correct_predictions = 0\n",
    "        val_total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in validation_dataloader:\n",
    "                val_inputs = {key: value.to(device) for key, value in val_batch.items() if key != 'label'}\n",
    "                val_labels = val_batch['label'].to(device)\n",
    "\n",
    "                val_outputs = my_model(val_inputs)\n",
    "                val_probabilities = torch.nn.functional.softmax(val_outputs, dim=1)\n",
    "                val_predictions = torch.argmax(val_probabilities, dim=1)\n",
    "                val_correct_predictions += (val_predictions == val_labels).sum().item()\n",
    "                val_total_predictions += val_labels.shape[0]\n",
    "                val_inputs = None\n",
    "                val_labels = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        val_accuracy = val_correct_predictions / val_total_predictions\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered. Training stopped.\")\n",
    "                break\n",
    "\n",
    "        my_model.train()\n",
    "    \n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'models/parenting_model.pth'\n",
    "parenting_bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    parenting_model = MyModel(parenting_bert_model)\n",
    "    parenting_model.load_state_dict(torch.load(model_file))\n",
    "    parenting_model = parenting_model.to(device)\n",
    "else:\n",
    "    parenting_model = train_model(parenting_bert_model, parenting_train_dataloader, parenting_val_dataloader)\n",
    "    torch.save(parenting_model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'models/gun_model.pth'\n",
    "gun_bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    gun_model = MyModel(parenting_bert_model)\n",
    "    gun_model.load_state_dict(torch.load(model_file))\n",
    "    gun_model = gun_model.to(device)\n",
    "else:\n",
    "    gun_model = train_model(gun_bert_model, gun_train_dataloader, gun_val_dataloader)\n",
    "    torch.save(gun_model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeToo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'models/metoo_model.pth'\n",
    "metoo_bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    metoo_model = MyModel(metoo_bert_model)\n",
    "    metoo_model.load_state_dict(torch.load(model_file))\n",
    "    metoo_model = metoo_model.to(device)\n",
    "else:\n",
    "    metoo_model= train_model(metoo_bert_model, metoo_train_dataloader, metoo_val_dataloader)\n",
    "    torch.save(metoo_model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(test, pred):\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(test, pred))\n",
    "    cm = confusion_matrix(test, pred, labels=[0, 1])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not spam', 'spam'])\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(my_model, test_dataloader):\n",
    "    my_model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_batch in test_dataloader:\n",
    "            test_inputs = {key: value.to(device) for key, value in test_batch.items() if key != 'label'}\n",
    "            test_labels = test_batch['label'].to(device)\n",
    "\n",
    "            test_outputs = my_model(test_inputs)\n",
    "            test_probabilities = torch.nn.functional.softmax(test_outputs, dim=1)\n",
    "            test_predictions = torch.argmax(test_probabilities, dim=1)\n",
    "\n",
    "            true_labels.extend(test_labels.cpu().numpy())\n",
    "            predicted_labels.extend(test_predictions.cpu().numpy())\n",
    "            test_inputs = None\n",
    "            test_labels = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    report(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(parenting_model, parenting_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(parenting_test['label'], parenting_rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(gun_model, gun_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(gun_test['label'], gun_rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeToo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(metoo_model, metoo_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(metoo_test['label'], metoo_rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parenting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gun Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(parenting_model, gun_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MeToo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(parenting_model, metoo_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gun Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parenting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(gun_model, parenting_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MeToo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(gun_model, metoo_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeToo Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gun Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(metoo_model, gun_test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parenting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(metoo_model, parenting_test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
